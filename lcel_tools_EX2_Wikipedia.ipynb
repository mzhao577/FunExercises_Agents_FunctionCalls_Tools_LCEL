{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b815ef73",
   "metadata": {},
   "source": [
    "# Tools and Routing\n",
    "\n",
    "Previously, we showed how functions can be supplied to OpenAI models. \n",
    "\n",
    "They can be used to specify an output schema required for tasks (e.g., extraction, tagging) or for an API. \n",
    "\n",
    "Tools are more general and are a central concept in [LLM agents](https://python.langchain.com/docs/use_cases/more/agents/).\n",
    "\n",
    "Often times, tools wrap APIs so that LLMs can easily access them. \n",
    "\n",
    "We can define tools (e.g., from functions) as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8988e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####.  Please use VirttualEnv: LCEL_extracting\n",
    "####.  Please use VirttualEnv: LCEL_extracting\n",
    "####.  Please use VirttualEnv: LCEL_extracting\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) \n",
    "OPENAI_API_KEY=os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecac1233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain==0.2.15\n",
      "langchain-community==0.2.13\n",
      "langchain-core==0.2.41\n",
      "langchain-openai==0.1.23\n",
      "langchain-text-splitters==0.2.4\n",
      "langchain-openai==0.1.23\n",
      "openai==1.47.0\n",
      "openapi-schema-pydantic==1.2.4\n",
      "pydantic==2.9.2\n",
      "pydantic-settings==2.5.2\n",
      "pydantic_core==2.23.4\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep langchain\n",
    "\n",
    "!pip freeze | grep openai\n",
    "\n",
    "!pip freeze | grep pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec49dd9e-b3e2-42b6-b203-2faa1206b2bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.agents import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef3ba5-88e7-4817-a369-e7343b3d830d",
   "metadata": {},
   "source": [
    "### Wikipedia Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f413308-dfc7-491f-b97c-9c2d711caf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in /Users/maxzhao/miniconda3/envs/lcel_extracting/lib/python3.12/site-packages (1.4.0)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/maxzhao/miniconda3/envs/lcel_extracting/lib/python3.12/site-packages (from wikipedia) (4.12.3)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/maxzhao/miniconda3/envs/lcel_extracting/lib/python3.12/site-packages (from wikipedia) (2.32.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/maxzhao/miniconda3/envs/lcel_extracting/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/maxzhao/miniconda3/envs/lcel_extracting/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/maxzhao/miniconda3/envs/lcel_extracting/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/maxzhao/miniconda3/envs/lcel_extracting/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/maxzhao/miniconda3/envs/lcel_extracting/lib/python3.12/site-packages (from beautifulsoup4->wikipedia) (2.6)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36ef8068-5fb5-41d5-82cb-d23cd04a7e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from langchain.agents import tool \n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    page_titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "    for page_title in page_titles[: 3]:\n",
    "        try:\n",
    "            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n",
    "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
    "        except (\n",
    "            self.wiki_client.exceptions.PageError,\n",
    "            self.wiki_client.exceptions.DisambiguationError,\n",
    "        ):\n",
    "            pass\n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    return \"\\n\\n\".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1aa241d-e37f-4dc1-adfc-5e059dc7b911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'search_wikipedia'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_wikipedia.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60b88614-9072-4b4d-ac85-06773db0bc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Run Wikipedia search and get page summaries.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_wikipedia.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d5881a9-16dc-4783-bb82-d2c8a12cacef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Page: LangChain\\nSummary: LangChain is a framework designed to simplify the creation of applications that utilize large language models (LLMs). As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\nPage: Retrieval-augmented generation\\nSummary: Retrieval augmented generation (RAG) is a type of generative artificial intelligence that has information retrieval capabilities. It modifies interactions with a large language model (LLM) so that the model responds to user queries with reference to a specified set of documents, using this information in preference to information drawn from its own vast, static training data. This allows LLMs to use domain-specific and/or updated information.  \\nUse cases include providing chatbot access to internal company data, or giving factual information only from an authoritative source.\\n\\n\\n\\nPage: Sentence embedding\\nSummary: In natural language processing, a sentence embedding refers to a numeric representation of a sentence in the form of a vector of real numbers which encodes meaningful semantic information.\\nState of the art embeddings are based on the learned hidden layer representation of dedicated sentence transformer models. BERT pioneered an approach involving the use of a dedicated [CLS] token prepended to the beginning of each sentence inputted into the model; the final hidden state vector of this token encodes information about the sentence and can be fine-tuned for use in sentence classification tasks. In practice however, BERT's sentence embedding with the [CLS] token achieves poor performance, often worse than simply averaging non-contextual word embeddings. SBERT later achieved superior sentence embedding performance by fine tuning BERT's [CLS] token embeddings through the usage of a siamese neural network architecture on the SNLI dataset. \\nOther approaches are loosely based on the idea of distributional semantics applied to sentences. Skip-Thought trains an encoder-decoder structure for the task of neighboring sentences predictions. Though this has been shown to achieve worse performance than approaches such as InferSent or SBERT. \\nAn alternative direction is to aggregate word embeddings, such as those returned by Word2vec, into sentence embeddings. The most straightforward approach is to simply compute the average of word vectors, known as continuous bag-of-words (CBOW). However, more elaborate solutions based on word vector quantization have also been proposed. One such approach is the vector of locally aggregated word embeddings (VLAWE), which demonstrated performance improvements in downstream text classification tasks.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_wikipedia.run({\"query\": \"langchain?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db928d-97f8-4389-91f1-6354824a9208",
   "metadata": {},
   "source": [
    "# Querying an API from a OpenAPI specfiction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a353363e-27b8-440f-a8b7-31bc2e861a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181b3737-eb4d-452a-b018-a1289f7f0d41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcel_extracting",
   "language": "python",
   "name": "lcel_extracting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
